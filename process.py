# -*- coding: utf-8 -*-
"""process.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uWwZ1Y_YXRPuIJJDIq8txoaN6_Fu-_cr
"""

import json
import re
from typing import Union

class Preprocessor:
    def __init__(self, file_path: str, chunk_size: int = 150) -> None:
        '''
        Initializes the Preprocessor with a file path and chunk size.

        Args:
            file_path: Path to the JSON file containing content to process
            chunk_size: Target number of words per content chunk (default: 150)

        Initializes:
            section_header: Marker text for relevant content sections (default: "Questions?")
            data: Processed content from the input file
            chunks: Final chunked content ready for use
        '''
        self.section_header = "Questions?"
        self.data = self.preprocess(file_path)
        self.chunks = self.content_chunks(chunk_size)

    def preprocess(self, file_path: str) -> list[dict[str, Union[str, dict]]]:
        '''
        Loads and preprocesses JSON content from the specified file.

        Args:
            file_path: Path to JSON file containing raw content

        Returns:
            List of dictionaries containing cleaned content and metadata.
            Each dictionary has:
            - 'content': Cleaned text content
            - 'metadata': Dictionary with 'link' and 'title' information

        Processing steps:
            1. Loads JSON data
            2. Cleans text (removes special chars, normalizes whitespace)
            3. Extracts content after section_header markers
        '''
        with open(file_path, 'r') as f:
            data = json.load(f)

        flag = False
        word_to_find = self.section_header
        processed_content = []

        for link, link_info in data.items():
            content = link_info.get('content', '')
            content = content.replace('\n', ' ')
            content = re.sub(r"[^\u0590-\u05FFa-zA-Z0-9.,!?;:'\"()\-\s]", "", content)
            content = re.sub(r'\s+', ' ', content).strip()

            if content:
                questions_index = content.find(word_to_find)
                if questions_index != -1:
                    if flag:
                        content = content[questions_index + len(word_to_find):]
                    else:
                        flag = True
                    processed_data = {
                        "content": content,
                        "metadata": {
                            "link": link,
                            "title": link_info.get('title', '')
                        }
                    }
                    processed_content.append(processed_data)

        return processed_content

    def content_chunks(self, chunk_size: int = 150) -> list[dict]:
        '''
        Splits preprocessed content into semantically meaningful chunks.

        Args:
            chunk_size: Target words per chunk (default: 150)

        Returns:
            List of dictionaries where each contains:
            - 'content': A text chunk
            - 'metadata': Original metadata from preprocessing

        Note:
            Uses chunk_content() for the actual splitting logic
        '''
        chunks = []
        for i in self.data:
            content = i['content']
            metadata = i['metadata']
            chunks.extend(self.chunk_content(content, chunk_size, metadata))
        return chunks

    def chunk_content(self, content: str, chunk_size: int = 150, metadata: dict = None) -> list[dict]:
        '''
        Splits content into chunks while respecting sentence boundaries.

        Args:
            content: Text content to split
            chunk_size: Target words per chunk (default: 150)
            metadata: Dictionary of metadata to attach to each chunk

        Returns:
            List of dictionaries with chunked content and metadata

        Chunking logic:
            1. Splits content into word lists
            2. Finds natural sentence endings within chunk size limits
            3. Preserves metadata with each chunk
            4. Ensures chunks don't break mid-sentence
        '''
        words = content.split()
        chunks = []
        start = 0
        while start < len(words):
            end = min(start + chunk_size, len(words))
            chunk = words[start:end]
            # Find the last word ending in a sentence (not a question!)
            split_idx = len(chunk)
            for i in reversed(range(len(chunk))):
                if chunk[i].endswith(('.', '!')):
                    split_idx = i + 1
                    break
            # Use the refined split point
            refined_chunk = chunk[:split_idx]
            chunks.append(" ".join(refined_chunk))
            # Move the pointer forward
            start += split_idx if split_idx > 0 else chunk_size
        for i in range(len(chunks)):
            chunks[i] = {'content': chunks[i], 'metadata': metadata}
        return chunks